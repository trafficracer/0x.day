{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tenseal pandas scikit-learn torch torchvision --upgrade\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tenseal as ts\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/output_file.csv\")\n",
        "\n",
        "# Preprocess the data\n",
        "df['age'] = df['age'] / 365.25\n",
        "df = df.drop(columns=['Name', 'Phone Number'])\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_columns = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
        "encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# Normalize numerical columns\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=['cardio'])\n",
        "y = df['cardio']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Create a TenSEAL context for CKKS\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        ")\n",
        "context.generate_galois_keys()\n",
        "context.global_scale = 2**40\n",
        "\n",
        "# Encrypt the model coefficients (weights) and intercept\n",
        "encrypted_weights = ts.ckks_vector(context, model.coef_[0])\n",
        "encrypted_intercept = ts.ckks_vector(context, [model.intercept_[0]])\n",
        "\n",
        "# Encrypt the test data\n",
        "encrypted_X_test = [ts.ckks_vector(context, row) for row in X_test.values]\n",
        "\n",
        "# Batched encrypted inference\n",
        "batch_size = 10  # Process 10 rows at a time\n",
        "encrypted_predictions = []\n",
        "\n",
        "for i in range(0, len(encrypted_X_test), batch_size):\n",
        "    batch = encrypted_X_test[i:i+batch_size]\n",
        "    for enc_row in batch:\n",
        "        enc_pred = enc_row.dot(encrypted_weights)  # Dot product\n",
        "        enc_pred += encrypted_intercept  # Add intercept\n",
        "        encrypted_predictions.append(enc_pred)\n",
        "\n",
        "# Decrypt predictions\n",
        "decrypted_predictions = [enc_pred.decrypt() for enc_pred in encrypted_predictions]\n",
        "\n",
        "# Convert predictions to binary classification\n",
        "final_predictions = [1 if pred[0] > 0.5 else 0 for pred in decrypted_predictions]\n",
        "\n",
        "# Print final predictions\n",
        "print(\"Predictions:\", final_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPjoIUa0i4nA",
        "outputId": "9204118f-70c2-4238-bbf3-9eddc7a11f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenseal in /usr/local/lib/python3.10/dist-packages (0.3.15)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tenseal as ts\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "df = pd.read_csv(\"/content/output_file.csv\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "# Convert 'age' from days to years\n",
        "df['age'] = df['age'] / 365.25\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df = df.drop(columns=['Name', 'Phone Number'])\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_columns = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
        "encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# Normalize numerical columns\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(columns=['cardio']).values\n",
        "y = df['cardio'].values\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Train a Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Setup Homomorphic Encryption Context\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        ")\n",
        "context.global_scale = 2**40\n",
        "context.generate_galois_keys()\n",
        "\n",
        "# Step 4: Batch Encrypt Data and Model Parameters\n",
        "def encrypt_batch(data, context):\n",
        "    \"\"\"Encrypts a batch of data rows.\"\"\"\n",
        "    return [ts.ckks_vector(context, row.tolist()) for row in data]\n",
        "\n",
        "encrypted_X_test = encrypt_batch(X_test, context)\n",
        "encrypted_weights = ts.ckks_vector(context, model.coef_[0])\n",
        "encrypted_bias = ts.ckks_vector(context, [model.intercept_[0]])\n",
        "\n",
        "# Step 5: Homomorphic Inference with Batch Processing\n",
        "encrypted_predictions = []\n",
        "for enc_row in encrypted_X_test:\n",
        "    enc_result = enc_row.dot(encrypted_weights) + encrypted_bias\n",
        "    encrypted_predictions.append(enc_result)\n",
        "\n",
        "# Step 6: Decrypt Predictions\n",
        "decrypted_predictions = [enc_pred.decrypt()[0] for enc_pred in encrypted_predictions]\n",
        "final_predictions = [1 if pred > 0.5 else 0 for pred in decrypted_predictions]\n",
        "\n",
        "# Step 7: Evaluate the Results\n",
        "print(f\"Decrypted Predictions: {decrypted_predictions}\")\n",
        "print(f\"Final Predictions: {final_predictions}\")\n",
        "accuracy = np.mean(np.array(final_predictions) == y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "AwKhmAXumSKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install tenseal pandas scikit-learn torch torchvision --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50u9LAWT8Fk2",
        "outputId": "608eef9d-6628-48a8-e585-85bbbef0e1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tenseal\n",
            "  Downloading tenseal-0.3.15-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cpu)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading tenseal-0.3.15-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenseal, pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 tenseal-0.3.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tenseal as ts\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "df = pd.read_csv(\"/content/output_file.csv\")\n",
        "\n",
        "# Convert 'age' from days to years\n",
        "df['age'] = df['age'] / 365.25\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df = df.drop(columns=['Name', 'Phone Number'])\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_columns = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
        "encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# Normalize numerical columns\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(columns=['cardio']).values\n",
        "y = df['cardio'].values\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Train a Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Setup Homomorphic Encryption Context\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        ")\n",
        "context.global_scale = 2**40\n",
        "context.generate_galois_keys()\n",
        "\n",
        "# Step 4: Batch Encrypt Data and Model Parameters\n",
        "def encrypt_batch(data, context, batch_size=500):\n",
        "    \"\"\"Encrypts data in batches.\"\"\"\n",
        "    encrypted_data = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        encrypted_batch = [ts.ckks_vector(context, row.tolist()) for row in batch]\n",
        "        encrypted_data.append(encrypted_batch)\n",
        "    return encrypted_data\n",
        "\n",
        "# Encrypt X_test in batches\n",
        "encrypted_X_test_batches = encrypt_batch(X_test, context)\n",
        "\n",
        "# Encrypt model weights and bias\n",
        "encrypted_weights = ts.ckks_vector(context, model.coef_[0].tolist())\n",
        "encrypted_bias = ts.ckks_vector(context, [model.intercept_[0]])\n",
        "\n",
        "# Step 5: Homomorphic Inference with Batch Processing\n",
        "def homomorphic_inference(encrypted_X_batches, encrypted_weights, encrypted_bias):\n",
        "    encrypted_predictions = []\n",
        "    for encrypted_X_batch in encrypted_X_batches:\n",
        "        batch_predictions = []\n",
        "        for enc_row in encrypted_X_batch:\n",
        "            enc_result = enc_row.dot(encrypted_weights) + encrypted_bias\n",
        "            batch_predictions.append(enc_result)\n",
        "        encrypted_predictions.append(batch_predictions)\n",
        "    return encrypted_predictions\n",
        "\n",
        "# Perform inference on the encrypted test batches\n",
        "encrypted_predictions_batches = homomorphic_inference(\n",
        "    encrypted_X_test_batches, encrypted_weights, encrypted_bias\n",
        ")\n",
        "\n",
        "# Step 6: Decrypt Predictions\n",
        "def decrypt_predictions(encrypted_predictions):\n",
        "    decrypted_predictions = []\n",
        "    for batch in encrypted_predictions:\n",
        "        decrypted_batch = [enc_pred.decrypt()[0] for enc_pred in batch]\n",
        "        decrypted_predictions.extend(decrypted_batch)\n",
        "    return decrypted_predictions\n",
        "\n",
        "# Decrypt the predictions\n",
        "decrypted_predictions = decrypt_predictions(encrypted_predictions_batches)\n",
        "\n",
        "# Step 7: Evaluate the Results\n",
        "final_predictions = [1 if pred > 0.5 else 0 for pred in decrypted_predictions]\n",
        "accuracy = np.mean(np.array(final_predictions) == y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi1NZSz48Am6",
        "outputId": "0d446b4c-c662-43fd-dbb1-858d76bdd496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 67.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "df = pd.read_csv(\"/content/output_file.csv\")\n",
        "\n",
        "# Convert 'age' from days to years\n",
        "df['age'] = df['age'] / 365.25\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df = df.drop(columns=['Name', 'Phone Number'])\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_columns = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
        "encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# Normalize numerical columns\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(columns=['cardio']).values\n",
        "y = df['cardio'].values\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Train a Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Evaluate the Model\n",
        "predictions = model.predict(X_test)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueQffSe0Bbkx",
        "outputId": "f968d02a-3672-46de-8c6e-01530852b49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tenseal as ts\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load and preprocess the dataset\n",
        "df = pd.read_csv(\"/content/output_file.csv\")\n",
        "\n",
        "# Convert 'age' from days to years\n",
        "df['age'] = df['age'] / 365.25\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df = df.drop(columns=['Name', 'Phone Number'])\n",
        "\n",
        "# Encode categorical columns\n",
        "categorical_columns = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
        "encoder = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# Normalize numerical columns\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop(columns=['cardio']).values\n",
        "y = df['cardio'].values\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Step 2: Define the Simple Neural Network Model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = X_train.shape[1]\n",
        "model = SimpleNN(input_size)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Step 3: Setup Homomorphic Encryption Context\n",
        "context = ts.context(\n",
        "    ts.SCHEME_TYPE.CKKS,\n",
        "    poly_modulus_degree=8192,\n",
        "    coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
        ")\n",
        "context.global_scale = 2**40\n",
        "context.generate_galois_keys()\n",
        "\n",
        "# Step 4: Encrypt Data and Model Parameters\n",
        "def encrypt_data(data, context):\n",
        "    \"\"\"Encrypts data using Tensail.\"\"\"\n",
        "    return [ts.ckks_vector(context, row.numpy().tolist()) for row in data]\n",
        "\n",
        "# Encrypt training and testing data\n",
        "encrypted_X_train = encrypt_data(X_train_tensor, context)\n",
        "encrypted_X_test = encrypt_data(X_test_tensor, context)\n",
        "\n",
        "# Encrypt model weights and bias\n",
        "encrypted_weights = ts.ckks_vector(context, model.fc.weight.data.numpy().flatten().tolist())\n",
        "encrypted_bias = ts.ckks_vector(context, model.fc.bias.data.numpy().tolist())\n",
        "\n",
        "# Step 5: Homomorphic Inference with Batch Processing\n",
        "def homomorphic_inference(encrypted_X_batches, encrypted_weights, encrypted_bias):\n",
        "    encrypted_predictions = []\n",
        "    for encrypted_X_batch in encrypted_X_batches:\n",
        "        batch_predictions = []\n",
        "        for enc_row in encrypted_X_batch:\n",
        "            enc_result = enc_row.dot(encrypted_weights) + encrypted_bias\n",
        "            batch_predictions.append(enc_result)\n",
        "        encrypted_predictions.append(batch_predictions)\n",
        "    return encrypted_predictions\n",
        "\n",
        "# Encrypt test data in batches\n",
        "def encrypt_batch(data, context, batch_size=500):\n",
        "    \"\"\"Encrypts data in batches.\"\"\"\n",
        "    encrypted_data = []\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        encrypted_batch = [ts.ckks_vector(context, row.tolist()) for row in batch]\n",
        "        encrypted_data.append(encrypted_batch)\n",
        "    return encrypted_data\n",
        "\n",
        "# Encrypt X_test in batches\n",
        "encrypted_X_test_batches = encrypt_batch(X_test_tensor, context)\n",
        "\n",
        "# Perform inference on the encrypted test batches\n",
        "encrypted_predictions_batches = homomorphic_inference(\n",
        "    encrypted_X_test_batches, encrypted_weights, encrypted_bias\n",
        ")\n",
        "\n",
        "# Step 6: Decrypt Predictions\n",
        "def decrypt_predictions(encrypted_predictions):\n",
        "    decrypted_predictions = []\n",
        "    for batch in encrypted_predictions:\n",
        "        decrypted_batch = [enc_pred.decrypt()[0] for enc_pred in batch]\n",
        "        decrypted_predictions.extend(decrypted_batch)\n",
        "    return decrypted_predictions\n",
        "\n",
        "# Decrypt the predictions\n",
        "decrypted_predictions = decrypt_predictions(encrypted_predictions_batches)\n",
        "\n",
        "# Step 7: Evaluate the Results\n",
        "final_predictions = [1 if pred > 0.5 else 0 for pred in decrypted_predictions]\n",
        "accuracy = np.mean(np.array(final_predictions) == y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQmtqeH5CkN8",
        "outputId": "cb6baca9-d52a-4f77-99ef-c5175576bc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.6845\n",
            "Epoch [20/100], Loss: 0.6818\n",
            "Epoch [30/100], Loss: 0.6794\n",
            "Epoch [40/100], Loss: 0.6771\n",
            "Epoch [50/100], Loss: 0.6749\n",
            "Epoch [60/100], Loss: 0.6729\n",
            "Epoch [70/100], Loss: 0.6710\n",
            "Epoch [80/100], Loss: 0.6692\n",
            "Epoch [90/100], Loss: 0.6676\n",
            "Epoch [100/100], Loss: 0.6660\n",
            "Accuracy: 51.29%\n"
          ]
        }
      ]
    }
  ]
}